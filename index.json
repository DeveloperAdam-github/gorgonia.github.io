[{"uri":"https://gorgonia.org/about/computation-graph/","title":"Computation Graph","tags":[],"description":"Graphs and *Nodes","content":" Gorgonia is Graph based Note: this article takes its inspiration from this blog post\nLike most deep learning libraries such as Tensorflow or Theano, Gorgonia rely on the concept that equations are representable by graphs.\nIt expose the equation graph as an ExprGraph object that can be manipulated by the programmer.\nSo instead of writing:\nfunc main() { fmt.Printf(\u0026#34;%v\u0026#34;, 1+1) } the programmer should write:\nfunc main() { // Create a graph. \tg := gorgonia.NewGraph() // Create a node called \u0026#34;x\u0026#34; with the value 1. \tx := gorgonia.NodeFromAny(g, 1, gorgonia.WithName(\u0026#34;x\u0026#34;)) // Create a node called \u0026#34;y\u0026#34; with the value 1. \ty := gorgonia.NodeFromAny(g, 1, gorgonia.WithName(\u0026#34;y\u0026#34;)) // z := x + y \tz := gorgonia.Must(gorgonia.Add(x, y)) // Create a VM to execute the graph. \tvm := gorgonia.NewTapeMachine(g) // Run the VM. Errors are not checked. \tvm.RunAll() // Print the value of z. \tfmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) } Numerical stability Consider the equation $y = log(1+x)$. This equation is not numerically stable - for very small values of $x$, the answer will most likely be wrong. This is because of the way float64 is designed - a float64 does not have enough bits to be able to tell apart 1 and 1 + 10e-16. In fact, the correct way to do it in Go is to use the built in library function math.Log1p. It can be shown in this simple program:\nfunc main() { fmt.Printf(\u0026#34;%v\\n\u0026#34;, math.Log(1.0+10e-16)) fmt.Printf(\u0026#34;%v\\n\u0026#34;, math.Log1p(10e-16)) }1.110223024625156e-15 // wrong 9.999999999999995e-16 // correct Gorgonia takes care of this using the best implementation to assure numerical stability.\nExpGraph and *Node The ExprGraph is the object holding the equation. This vertices of this graph are the values or operators that compose the equation we want to materialize. Those vertices are represented by a structure called \u0026ldquo;Node\u0026rdquo;. The graph holds pointer to this structure.\nTo create the equation, we need to create an ExprGraph, add some *Nodes, it and linked them together.\nLuckily, we don\u0026rsquo;t have to manage the connections between the nodes manually.\nPlaceholders and Operators The Node can hold some Values (a Value is a Go interface that represents a concrete type such as a scalar or a tensor). But it can also hold Operators.\nAt computation time, the values will flow along the graphs and each node containing an Operator will execute the corresponding code and set the value to the corresponding node.\nGradient computation On top of that, Gorgonia can do both symbolic and automatic differentiation. This page explains how it works in detail.\n"},{"uri":"https://gorgonia.org/reference/exprgraph/","title":"Graph / Exprgraph","tags":[],"description":"","content":"A lot has been said about a computation graph or an expression graph. But what is it exactly? Think of it as an AST for the math expression that you want. Here\u0026rsquo;s the graph for the examples (but with a vector and a scalar addition instead) above:\nBy the way, Gorgonia comes with nice-ish graph printing abilities. Here\u0026rsquo;s an example of a graph of the equation $y = x^2$ and its derivation:\nTo read the graph is easy. The expression builds from bottom up, while the derivations build from top down. This way the derivative of each node is roughly on the same level.\nRed-outlined nodes indicate that it\u0026rsquo;s a root node. Green outlined nodes indicate that they\u0026rsquo;re a leaf node. Nodes with a yellow background indicate that it\u0026rsquo;s an input node. The dotted arrows indicate which node is the gradient node for the pointed-to node.\nConcretely, it says that c42011e840 ($\\frac{\\partial{y}}{\\partial{x}}$) is the gradient node of the input c42011e000 (which is $x$).\n"},{"uri":"https://gorgonia.org/tutorials/hello-world/","title":"Hello World","tags":[],"description":"","content":" This is a step by step tutorial to do a very simple computation with Gorgonia.\nOur goal is to use all the plumbing of Gorgonia to do a simple operation:\n$ f(x,y) = x + y $\nwith x = 2 and y = 5\nhow it works The equation x + y = z can be represented as a graph:\ngraph LR; z[z] -- add(Round edge) add[+] -- x add[+] -- y  To compute the result, we use 4 steps:\n Make a similar graph with Gorgonia sets some values on the nodes x and y then instanciate a graph on a gorgonia vm extract the value from node z *  Create a graph Create an empty expression graph with this method:\ng := gorgonia.NewGraph() Create the nodes We will create some nodes and associate them to the ExprGraph.\nvar x, y, z *gorgonia.Node Create the placeholder x and y are scalar variables, we can create the corresponding node with:\nx = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) the functions take the exprgraph as argument; the resulting node is automatically associated to the graph.\n Now create the addition operator; this operator takes two nodes and returns a new node z:\nif z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } the returning node z is added to the graph even if g is not passed to z or to the Add function.\n Set the values We have a ExprGraph that represents the equation z = x + y. Now it\u0026rsquo;s time to assign some values to x and y.\nWe use the Let function:\ngorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) Run the graph To run the graph and compute the result, we need to instanciate a VM. Let\u0026rsquo;s use the TapeMachine:\nmachine := gorgonia.NewTapeMachine(g) defer machine.Close() and run the graph:\nif err = machine.RunAll(); err != nil { log.Fatal(err) } If a second run is needed, it is mandatory to call the Reset() method of the vm object: machine.Reset()\n Get the result Now the node z holds the result. We can extract its value by calling the Value() method:\nfmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) we could also access the underlying \u0026ldquo;Go\u0026rdquo; value with a call to z.Value().Data() which would return an interface{} holding a float64 in our case\n Final result package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // define the expression  x = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() // set initial values then run  gorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) }$ go run main.go 4.5"},{"uri":"https://gorgonia.org/tutorials/mnist/","title":"Building Simple Neural Net (MNIST)","tags":[],"description":"","content":"TODO\n"},{"uri":"https://gorgonia.org/getting-started/","title":"Getting Started","tags":[],"description":"Quick start with Gorgonia","content":" Getting gorgonia Gorgonia is go-gettable and supports go modules. To get the library and its dependencies, simply run\n$ go get gorgonia.org/gorgonia First code to do a simple computation create a simple program to see if the plumbing is ok:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // define the expression  x = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() // set initial values then run  gorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) } running the program should print the result: 4.5\nFor further explanation, please see the Hello World tutorial.\n"},{"uri":"https://gorgonia.org/about/","title":"How Gorgonia works","tags":[],"description":"Articles with a goal to explain how gorgonia works.","content":" about Gorgonia works by creating a computation graph, and then executing it. Think of it as a programming language, but is limited to mathematical functions, and has no branching capability (no if/then or loops). In fact this is the dominant paradigm that the user should be used to thinking about. The computation graph is an AST.\nMicrosoft\u0026rsquo;s CNTK, with its BrainScript, is perhaps the best at exemplifying the idea that building of a computation graph and running of the computation graphs are different things, and that the user should be in different modes of thoughts when going about them.\nWhilst Gorgonia\u0026rsquo;s implementation doesn\u0026rsquo;t enforce the separation of thought as far as CNTK\u0026rsquo;s BrainScript does, the syntax does help a little bit.\ngoing further This chapter contains articles with a goal to explain how gorgonia works.\nThe articles in this section are understanding-oriented, and provides background and context.\n  Computation Graph  Graphs and *Nodes\n Differentiation  How gradient computation works within Gorgonia\n "},{"uri":"https://gorgonia.org/about/differentiation/","title":"Differentiation","tags":[],"description":"How gradient computation works within Gorgonia","content":" about This section is a work in progress and explains how differentiation works\n Automatic Differentiation  This page will explain how automatic differentiation works\n Symbolic Differentiation  This page will explain how symbolic differentiation works\n "},{"uri":"https://gorgonia.org/tutorials/","title":"Tutorials","tags":[],"description":"tutorials on various use-cases","content":" Tutorials Various tutorials to start with various usage of Gorgonia.\nThose tutorials are:\n learning-oriented allows the newcomer to get started are a lesson  Analogy: teaching a small child how to cook\nTutorials available so far  Hello World   Building Simple Neural Net (MNIST)   Multivariate linear regression on Iris Dataset   "},{"uri":"https://gorgonia.org/how-to/","title":"How To","tags":[],"description":"Various howto solve a specific problem with Gorgonia","content":"How to do different machine-learning things with Gorgonia.\nIn this section you will see how Gorgonia can be used to solve various problems.\nThose how-to guides:\n are goal-oriented shows how to solve a specific problem are made of understandable steps  Analogy: a recipe in a cookery book\n Create a tensor from a Dataframe (gota)   Save Weights   "},{"uri":"https://gorgonia.org/reference/","title":"Reference guide","tags":[],"description":"This is a reference guide of Gorgonia. It describes the machinery","content":" Reference This is the reference guide of Gorgonia. The goal of the articles in this section are:\n being information-oriented describing the machinery being accurate and complete  Analogy: a reference encyclopaedia article\n Graph / Exprgraph   Present   Tensor   Solvers   VM   "},{"uri":"https://gorgonia.org/reference/present/","title":"Present","tags":[],"description":"","content":" This page contains material that can be used within presentation.\nLogos   Logos   logo_g.svg  (22 ko)   logo_g_square.png  (142 ko)   logo_horizontal.svg  (25 ko)   logo_vertical.svg  (25 ko)    "},{"uri":"https://gorgonia.org/misc/","title":"Miscellaneous","tags":[],"description":"","content":" Video related to Gorgonia  Gorgonia, A library that helps facilitate machine learning in Go - Sydney Go Meetup, September 2016 \u0026ldquo;A Funny Thing Happened On The Way To Reimplementing AlphaGo\u0026rdquo; (in Go) by Xuanyi Chew  Articles mentioning Gorgonia  Gorgonia (original post on Xuanyi Chew\u0026rsquo;s blog) Tensor Refactor: A Go Experience Report Think like a vertex: using Go\u0026rsquo;s concurrency for graph computation  "},{"uri":"https://gorgonia.org/tutorials/iris/","title":"Multivariate linear regression on Iris Dataset","tags":[],"description":"","content":" About We will use Gorgonia to create a linear regression model.\nThe goal is, to predict the species of the Iris flowers given the characteristics:\n sepal_length sepal_width petal_length petal_width  The species we want to predict are:\n setosa virginica versicolor  The goal of this tutorial is to use Gorgonia to find the correct values of $\\Theta$ given the iris dataset, in order to write a CLI utility that would look like this:\n./iris sepal length: 5 sepal width: 3.5 petal length: 1.4 sepal length: 0.2 It is probably a setosa This tutorial is for academic purpose. Its goal is to describe how to do this with Gorgonia; It is not the state of the art answer to this particular problem.\n Mathematical representation We will consider that the species of Iris if a function of its sepal length and width as well as its petal length and width.\nTherefore, if we consider that $y$ is the value of the species, we the equation we would like to solve is:\n$$ y = \\theta_0 + \\theta_1 * sepal\\_length + \\theta_2 * sepal\\_width + \\theta_3 * petal\\_length + \\theta_4 * petal\\_width$$\nLet\u0026rsquo;s consider the vectors $x$ and $\\Theta$ such as:\n$$ x = \\begin{bmatrix} sepal\\_length \u0026amp; sepal\\_width \u0026amp; petal\\_length \u0026amp; petal\\_width \u0026amp; 1\\end{bmatrix}$$\n$$ \\Theta = \\begin{bmatrix} \\theta_4 \\theta_3 \\theta_2 \\theta_1 \\theta_0 \\end{bmatrix} $$\nWe have\n$$y = x\\cdot\\Theta$$\nLinear regression To find the correct values, we will use a linear regression. We will encode the data (the true facts from observation of different flowers) into a matrix $X$ containing 5 columns (sepal length, sepal width, petal length, petal width and 1 for the bias). A row of the matrix represent a flower.\nThe we will encode the corresponding species into a column vector $Y$ with float values.\n setosa = 1.0 virginica = 2.0 versicolor = 3.0  In the learning phase, the cost is expressed like this:\n$cost = \\dfrac{1}{m} \\sum_{i=1}^m(X^{(i)}\\cdot\\Theta-Y^{(i)})^2$\nWe will use the gradient descent to lower the cost and get the accurate values for $\\Theta$\nIt is possible to get the exact $\\theta$ values with the Normal Equation $$ \\theta = \\left( X^TX \\right)^{-1}X^TY $$ See this gist for a basic implementation with Gonum.\n Generate the training set with gota (dataframe) First, let\u0026rsquo;s generate the training data. We use a dataframe to do this smoothly.\nSee this howto for more info about using the dataframe\n func getXYMat() (*mat.Dense, *mat.Dense) { f, err := os.Open(\u0026#34;iris.csv\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() df := dataframe.ReadCSV(f) xDF := df.Drop(\u0026#34;species\u0026#34;) toValue := func(s series.Series) series.Series { records := s.Records() floats := make([]float64, len(records)) for i, r := range records { switch r { case \u0026#34;setosa\u0026#34;: floats[i] = 1 case \u0026#34;virginica\u0026#34;: floats[i] = 2 case \u0026#34;versicolor\u0026#34;: floats[i] = 3 default: log.Fatalf(\u0026#34;unknown iris: %v\\n\u0026#34;, r) } } return series.Floats(floats) } yDF := df.Select(\u0026#34;species\u0026#34;).Capply(toValue) numRows, _ := xDF.Dims() xDF = xDF.Mutate(series.New(one(numRows), series.Float, \u0026#34;bias\u0026#34;)) fmt.Println(xDF.Describe()) fmt.Println(yDF.Describe()) return mat.DenseCopyOf(\u0026amp;matrix{xDF}), mat.DenseCopyOf(\u0026amp;matrix{yDF}) } This returns two matrices we can use in Gorgonia.\nCreate the expression graph The equation $X\\cdot\\Theta$ is represented as an ExprGraph:\nfunc getXY() (*tensor.Dense, *tensor.Dense) { x, y := getXYMat() xT := tensor.FromMat64(x) yT := tensor.FromMat64(y) // Get rid of the last dimension to create a vector \ts := yT.Shape() yT.Reshape(s[0]) return xT, yT } func main() { xT, yT := getXY() g := gorgonia.NewGraph() x := gorgonia.NodeFromAny(g, xT, gorgonia.WithName(\u0026#34;x\u0026#34;)) y := gorgonia.NodeFromAny(g, yT, gorgonia.WithName(\u0026#34;y\u0026#34;)) theta := gorgonia.NewVector( g, gorgonia.Float64, gorgonia.WithName(\u0026#34;theta\u0026#34;), gorgonia.WithShape(xT.Shape()[1]), gorgonia.WithInit(gorgonia.Uniform(0, 1))) pred := must(gorgonia.Mul(x, theta)) // Saving the value for later use  var predicted gorgonia.Value gorgonia.Read(pred, \u0026amp;predicted) Gorgonia is higly optimized; it heavily plays with pointers and memory to get good performances. Therefore, calling the Value() method of a *Node at runtime (during the execution process), may lead to incorrect results. If we need to access a specific value of a *Node at runtime (for example during the learning phase), we need to keep a reference to its underlying Value. This is why we use the Read method here. predicted will hold a Value containing the result of $X\\cdot\\Theta$ at anytime.\n Preparing the gradient computation We will use Gorgonia\u0026rsquo;s Symbolic differentiation capability.\nFirst, we will create the cost function, and use a solver to perform a gradient descent to lower the cost.\nCreate the node holding the cost We complete the exprgraph by adding the cost ($cost = \\dfrac{1}{m} \\sum_{i=1}^m(X^{(i)}\\cdot\\Theta-Y^{(i)})^2$)\nsquaredError := must(gorgonia.Square(must(gorgonia.Sub(pred, y)))) cost := must(gorgonia.Mean(squaredError)) We want to lower this cost, so we evaluate the gradient wrt to $\\Theta$:\nif _, err := gorgonia.Grad(cost, theta); err != nil { log.Fatalf(\u0026#34;Failed to backpropagate: %v\u0026#34;, err) } The gradient descent We are using the mechanism of the gradient descent. This means that we use the gradient to modulate the parameters $\\Theta$ step by step.\nThe basic gradient descent is implemented by Vanilla Solver of Gorgonia. We set the learning rate $\\gamma$ to be 0.001.\nsolver := gorgonia.NewVanillaSolver(gorgonia.WithLearnRate(0.001)) And at each step, we will ask the solver to update the $\\Theta$ parameters thanks to its gradient. Therefore, we set an update variable that we will pass to the solver at each iteration\nThe gradient descent will update the all the values passed into []gorgonia.ValueGrad at each step according this equation: ${\\displaystyle x^{(k+1)}=x^{(k)}-\\gamma \\nabla f\\left(x^{(k)}\\right)}$ It is important to understand that the solver works on Values and not on Nodes. But to make things easy, ValueGrad is an interface{} fulfilled by the *Node structure.\n In our case, we want to optimize $\\Theta$ and ask the solver will update its value like this:\n${\\displaystyle \\Theta^{(k+1)}=\\Theta^{(k)}-\\gamma \\nabla f\\left(\\Theta^{(k)}\\right)}$\nTo do so, we need to pass $\\Theta$ to the Step method of the Solver:\nupdate := []gorgonia.ValueGrad{theta} // ... if err = solver.Step(update); err != nil { log.Fatal(err) } The learning iterations Now that we have the principle, we need to run the computation with a vm several times so the gradient descent\u0026rsquo;s magic can happen.\nLet\u0026rsquo;s create a vm to execute the graph (and do the gradient computation):\nmachine := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(theta)) defer machine.Close() We will ask the solver to update the parameter $\\Theta$ wrt to its gradient. Therefore we must instruct the TapeMachine to store the value of $\\Theta$ as well as its derivative (its dual value). We do this with the BindDualValues function.\n Now let\u0026rsquo;s create the loop and execute the graph at each step; the machine will learn!\niter := 1000000 var err error for i := 0; i \u0026lt; iter; i++ { if err = machine.RunAll(); err != nil { fmt.Printf(\u0026#34;Error during iteration: %v: %v\\n\u0026#34;, i, err) break } if err = solver.Step(model); err != nil { log.Fatal(err) } machine.Reset() // Reset is necessary in a loop like this } Getting some infos We can dump some info about the learning process by using this call\nfmt.Printf(\u0026#34;theta: %2.2f Iter: %v Cost: %2.3f Accuracy: %2.2f \\r\u0026#34;, theta.Value(), i, cost.Value(), accuracy(predicted.Data().([]float64), y.Value().Data().([]float64))) with accuracy defined like this:\nfunc accuracy(prediction, y []float64) float64 { var ok float64 for i := 0; i \u0026lt; len(prediction); i++ { if math.Round(prediction[i]-y[i]) == 0 { ok += 1.0 } } return ok / float64(len(y)) } This will display a line like this during the learning process:\ntheta: [ 0.26 -0.41 0.44 -0.62 0.83] Iter: 26075 Cost: 0.339 Accuracy: 0.61 Save the weights Once the training is done, we save the values of $\\Theta$ to be able to do some predictions:\nfunc save(value gorgonia.Value) error { f, err := os.Create(\u0026#34;theta.bin\u0026#34;) if err != nil { return err } defer f.Close() enc := gob.NewEncoder(f) err = enc.Encode(value) if err != nil { return err } return nil } Create a simple CLI for predictions First, let\u0026rsquo;s load the parameters from the training phase:\nfunc main() { f, err := os.Open(\u0026#34;theta.bin\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() dec := gob.NewDecoder(f) var thetaT *tensor.Dense err = dec.Decode(\u0026amp;thetaT) if err != nil { log.Fatal(err) } Then, let\u0026rsquo;s create the model (the exprgraph) like we did before:\nA real application would probably have shared the model in a separate package\n g := gorgonia.NewGraph() theta := gorgonia.NodeFromAny(g, thetaT, gorgonia.WithName(\u0026#34;theta\u0026#34;)) values := make([]float64, 5) xT := tensor.New(tensor.WithBacking(values)) x := gorgonia.NodeFromAny(g, xT, gorgonia.WithName(\u0026#34;x\u0026#34;)) y, err := gorgonia.Mul(x, theta) Then enter a for loop that will get info from stdin, do the computation and display the result:\nmachine := gorgonia.NewTapeMachine(g) values[4] = 1.0 for { values[0] = getInput(\u0026#34;sepal length\u0026#34;) values[1] = getInput(\u0026#34;sepal width\u0026#34;) values[2] = getInput(\u0026#34;petal length\u0026#34;) values[3] = getInput(\u0026#34;petal width\u0026#34;) if err = machine.RunAll(); err != nil { log.Fatal(err) } switch math.Round(y.Value().Data().(float64)) { case 1: fmt.Println(\u0026#34;It is probably a setosa\u0026#34;) case 2: fmt.Println(\u0026#34;It is probably a virginica\u0026#34;) case 3: fmt.Println(\u0026#34;It is probably a versicolor\u0026#34;) default: fmt.Println(\u0026#34;unknown iris\u0026#34;) } machine.Reset() } This is a helper function to get the input:\nfunc getInput(s string) float64 { reader := bufio.NewReader(os.Stdin) fmt.Printf(\u0026#34;%v: \u0026#34;, s) text, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) text = strings.Replace(text, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, -1) input, err := strconv.ParseFloat(text, 64) if err != nil { log.Fatal(err) } return input } Now we can go build or go run the code, and voilà! We have a fully autonomous CLI that can predict the iris species regarding its features:\n$ go run main.go sepal length: 4.4 sepal widt: 2.9 petal length: 1.4 petal width: 0.2 It is probably a setosa sepal length: 5.9 sepal widt: 3.0 petal length: 5.1 petal width: 1.8 It is probably a virginica Conclusion This is a step by step example. You can now play with the initialization values of theta, or change to solver to see how thing goes within Gorgonia.\nThe full code can be found in the example of the Gorgonia project.\nBonus: visual representation It is possible to visualize the dataset using the Gonum plotter libraries. Here is a simple example on how to achieve it:\nimport ( \u0026#34;gonum.org/v1/plot\u0026#34; \u0026#34;gonum.org/v1/plot/plotter\u0026#34; \u0026#34;gonum.org/v1/plot/plotutil\u0026#34; \u0026#34;gonum.org/v1/plot/vg\u0026#34; \u0026#34;gonum.org/v1/plot/vg/draw\u0026#34; ) func plotData(x []float64, a []float64) []byte { p, err := plot.New() if err != nil { log.Fatal(err) } p.Title.Text = \u0026#34;sepal length \u0026amp; width\u0026#34; p.X.Label.Text = \u0026#34;length\u0026#34; p.Y.Label.Text = \u0026#34;width\u0026#34; p.Add(plotter.NewGrid()) l := len(x) / len(a) for k := 1; k \u0026lt;= 3; k++ { data0 := make(plotter.XYs, 0) for i := 0; i \u0026lt; len(a); i++ { if k != int(a[i]) { continue } x1 := x[i*l+0] // sepal_length \ty1 := x[i*l+1] // sepal_width \tdata0 = append(data0, plotter.XY{X: x1, Y: y1}) } data, err := plotter.NewScatter(data0) if err != nil { log.Fatal(err) } data.GlyphStyle.Color = plotutil.Color(k - 1) data.Shape = \u0026amp;draw.PyramidGlyph{} p.Add(data) p.Legend.Add(fmt.Sprint(k), data) } w, err := p.WriterTo(4*vg.Inch, 4*vg.Inch, \u0026#34;png\u0026#34;) if err != nil { panic(err) } var b bytes.Buffer writer := bufio.NewWriter(\u0026amp;b) w.WriteTo(writer) ioutil.WriteFile(\u0026#34;out.png\u0026#34;, b.Bytes(), 0644) return b.Bytes() }"},{"uri":"https://gorgonia.org/how-to/dataframe/","title":"Create a tensor from a Dataframe (gota)","tags":[],"description":"","content":" This howto explains how to create a tensor from a dataframe using gota The goal is to read a csv file and create a *tensor.Dense with shape (2,2).\nCreate the dataframe from a csv file Consider a csv file with the following content:\nsepal_length,sepal_width,petal_length,petal_width,species 5.1 ,3.5 ,1.4 ,0.2 ,setosa 4.9 ,3.0 ,1.4 ,0.2 ,setosa 4.7 ,3.2 ,1.3 ,0.2 ,setosa 4.6 ,3.1 ,1.5 ,0.2 ,setosa 5.0 ,3.6 ,1.4 ,0.2 ,setosa ... This is extract from the Iris flower data set. A copy of the dataset can be found here\n We want to create a tensor with all values but the species.\nCreate the dataframe with gota. gota\u0026rsquo;s dataframe package has a function ReadCSV that takes an io.Reader as argument.\nf, err := os.Open(\u0026#34;iris.csv\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() df := dataframe.ReadCSV(f) df is a DataFrame that contains all the data present in the file.\ngota uses te first line of the CSV to reference the columns in the dataframe\n Let\u0026rsquo;s remove the species column:\nxDF := df.Drop(\u0026#34;species\u0026#34;) Convert the dataframe into a matrix To make things easier, we will convert our dataframe into a Matrix as defined by gonum (see the matrix godoc). matrix is an interface. gota\u0026rsquo;s dataframe does not fulfill the Matrix interface. As described into gota\u0026rsquo;s documentation, we create a wrapper around DataFrame to fulfil the Matrix interface.\ntype matrix struct { dataframe.DataFrame } func (m matrix) At(i, j int) float64 { return m.Elem(i, j).Float() } func (m matrix) T() mat.Matrix { return mat.Transpose{Matrix: m} } Create the tensor Now we can create a *Dense tensor thanks to the function tensor.FromMat64 by wrapping the dataframe into the matrix structure.\nxT := tensor.FromMat64(mat.DenseCopyOf(\u0026amp;matrix{xDF}))"},{"uri":"https://gorgonia.org/how-to/save-weights/","title":"Save Weights","tags":[],"description":"","content":" Goal The goal of this howto is to describe a way to save the values of the nodes and to restore them.\nImplementation The best thing you can do right now is to save the value of the corresponding nodes and restore them.\nThe tensors are fulfilling the GobEncode and GobDecode interface and this is the best option. You can also save the backend as a slice of elements but this is a little bit trickier.\nHere is a sample code to do so (it is not optimized at all, feel free to amend it):\npackage main import ( \u0026#34;encoding/gob\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; \u0026#34;gorgonia.org/tensor\u0026#34; ) var ( backup = \u0026#34;/tmp/example_gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // Create the graph  x = gorgonia.NewTensor(g, gorgonia.Float64, 2, gorgonia.WithShape(2, 2), gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewTensor(g, gorgonia.Float64, 2, gorgonia.WithShape(2, 2), gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // Init variables  xT, yT, err := readFromBackup() if err != nil { log.Println(\u0026#34;cannot read backup, doing init\u0026#34;, err) xT = tensor.NewDense(gorgonia.Float64, []int{2, 2}, tensor.WithBacking([]float64{0, 1, 2, 3})) yT = tensor.NewDense(gorgonia.Float64, []int{2, 2}, tensor.WithBacking([]float64{0, 1, 2, 3})) } err = gorgonia.Let(x, xT) if err != nil { log.Fatal(err) } err = gorgonia.Let(y, yT) if err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) err = save([]*gorgonia.Node{x, y}) if err != nil { log.Fatal(err) } } func readFromBackup() (tensor.Tensor, tensor.Tensor, error) { f, err := os.Open(backup) if err != nil { return nil, nil, err } defer f.Close() dec := gob.NewDecoder(f) var xT, yT *tensor.Dense log.Println(\u0026#34;decoding xT\u0026#34;) err = dec.Decode(\u0026amp;xT) if err != nil { return nil, nil, err } log.Println(\u0026#34;decoding yT\u0026#34;) err = dec.Decode(\u0026amp;yT) if err != nil { return nil, nil, err } return xT, yT, nil } func save(nodes []*gorgonia.Node) error { f, err := os.Create(backup) if err != nil { return err } defer f.Close() enc := gob.NewEncoder(f) for _, node := range nodes { err := enc.Encode(node.Value()) if err != nil { return err } } return nil } which gives:\n$ go run main.go 2019/10/28 08:07:26 cannot read backup, doing init open /tmp/example_gorgonia: no such file or directory ⎡0 2⎤ ⎣4 6⎦ $ go run main.go 2019/10/28 08:07:29 decoding xT 2019/10/28 08:07:29 decoding yT ⎡0 2⎤ ⎣4 6⎦"},{"uri":"https://gorgonia.org/reference/tensor/","title":"Tensor","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/reference/vm/lispmachine/","title":"LispMachine","tags":[],"description":"","content":"The LispMachine was designed to take a graph as an input, and executes directly on the nodes of the graph. If the graph change, simply create a new lightweight LispMachine to execute it on. The LispMachine is suitable for tasks such as creating recurrent neural networks without a fixed size.\nThe trade-off is that executing a graph on LispMachine is generally slower than on TapeMachine, given the same static \u0026ldquo;image\u0026rdquo; of a graph.\n"},{"uri":"https://gorgonia.org/reference/vm/tapemachine/","title":"Tapemachine","tags":[],"description":"","content":" The TapeMachine is useful for executing expressions that are generally static (that is to say the computation graph does not change). Due to its static nature, the TapeMachine is good for running expressions that are compiled-once-run-many-times (such as linear regression, SVM and the like).\nTechnical details The TapeMachine pre-compiles a graph into a list of instructions, then executes the instructions linearly and sequentially. The main trade-off is dynamism. Graphs cannot be dynamically created on the fly as a re-compilation process is required (and compilation is relatively expensive). However, graphs executed with the TapeMachine run much faster as plenty of optimizations has been done in the code generation stage.\n"},{"uri":"https://gorgonia.org/about/differentiation/autodiff/","title":"Automatic Differentiation","tags":[],"description":"","content":"This page will explain how automatic differentiation works\n"},{"uri":"https://gorgonia.org/about/differentiation/symbolicdiff/","title":"Symbolic Differentiation","tags":[],"description":"","content":"This page will explain how symbolic differentiation works\n"},{"uri":"https://gorgonia.org/reference/solver/","title":"Solvers","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/","title":"main","tags":[],"description":"","content":" Gorgonia Gorgonia is a library that helps facilitate machine learning in Go.\nWrite and evaluate mathematical equations involving multidimensional arrays easily.\nIf this sounds like PyTorch or TensorFlow, it\u0026rsquo;s because the idea is quite similar.\nSpecifically, the library is pretty low-level but has higher goals like Tensorflow.\nWhy use Gorgonia? The main reason to use Gorgonia is developer comfort. If you\u0026rsquo;re using a Go stack extensively, now you have access to the ability to create production-ready machine learning systems in an environment that you are already familiar and comfortable with.\nML/AI at large is usually split into two stages: the experimental stage where one builds various models, test and retest; and the deployed state where a model after being tested and played with, is deployed. This necessitate different roles like data scientist and data engineer.\nTypically the two phases have different tools: Python (PyTorch, etc) is commonly used for the experimental stage, and then the model is rewritten in some more performant language like C++ (using dlib, mlpack etc). Of course, nowadays the gap is closing and people frequently share the tools between them. Tensorflow is one such tool that bridges the gap.\nGorgonia aims to do the same, but for the Go environment. Gorgonia is currently fairly performant - its speeds are comparable to PyTorch\u0026rsquo;s and Tensorflow\u0026rsquo;s CPU implementations. GPU implementations are a bit finnicky to compare due to the heavy cgo tax, but rest assured that this is an area of active improvement.\nHow is this website organized? This website is composed of four sections with different goals:\n Getting Started  Quick start with Gorgonia\n How Gorgonia works  Articles with a goal to explain how gorgonia works.\n Tutorials  tutorials on various use-cases\n How To  Various howto solve a specific problem with Gorgonia\n Reference guide  This is a reference guide of Gorgonia. It describes the machinery\n Miscellaneous   Video related to Gorgonia Gorgonia, A library that helps facilitate machine learning in Go - Sydney Go Meetup, September 2016 \u0026ldquo;A Funny Thing Happened On The Way To Reimplementing AlphaGo\u0026rdquo; (in Go) by Xuanyi Chew Articles mentioning Gorgonia Gorgonia (original post on Xuanyi Chew\u0026rsquo;s blog) Tensor Refactor: A Go Experience Report Think like a vertex: using Go\u0026rsquo;s concurrency for graph computation  Vanity-import-paths  \n "},{"uri":"https://gorgonia.org/reference/vm/","title":"VM","tags":[],"description":"","content":"A VM in Gorgonia is object that understands the exprgraph and has implemented the ability to do computation with it.\nTechically speaking it is an interface{} with three methods:\ntype VM interface { RunAll() error Reset() // Close closes all the machine resources (CUDA, if any, loggers if any)  Close() error } There different VMs in the current version of Gorgonia:\n LispMachine   Tapemachine   They function differently and take different inputs.\n"},{"uri":"https://gorgonia.org/cu/","title":"CU","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/dawson/","title":"Dawson","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/golgi/","title":"Golgi","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/gorgonia/","title":"Gorgonia","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/randomkit/","title":"RandomKit","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/tensor/","title":"Tensor","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/vanity-import-paths/","title":"Vanity-import-paths","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/vecf32/","title":"vecf32","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/vecf64/","title":"vecf64","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/tags/","title":"Tags","tags":[],"description":"","content":""}]